"""
ç¦»çº¿è®­ç»ƒè„šæœ¬ - ä½¿ç”¨æœ¬åœ°FinBERTæ¨¡å‹ï¼Œå¸¦è¿›åº¦æ¡
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import json
import time
from datetime import datetime, timedelta
from model import MultimodalDecisionModel, DualMambaModel
from tqdm import tqdm
import sys

# é…ç½®
DATA_PATH = '../train_data/sequence_train_data.json'
BATCH_SIZE = 4  # å¤§å¹…é™ä½æ‰¹æ¬¡å¤§å°ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§
EPOCHS = 15  # å‡å°‘è½®æ•°ï¼Œé¿å…è¿‡é•¿è®­ç»ƒ
LR = 1e-5  # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ï¼Œç¡®ä¿ç¨³å®šè®­ç»ƒ
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
USE_DUAL_MAMBA = True

# GPUä¼˜åŒ–é…ç½®
MIXED_PRECISION = False  # æš‚æ—¶ç¦ç”¨æ··åˆç²¾åº¦ï¼Œä½¿ç”¨å…¨ç²¾åº¦æé«˜ç¨³å®šæ€§
GRADIENT_ACCUMULATION_STEPS = 4  # å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼Œè¡¥å¿å°æ‰¹æ¬¡çš„æ•ˆæœ

# æ ‡ç­¾æ˜ å°„
ACTION2IDX = {'å¢æŒ': 0, 'å‡æŒ': 1, 'è§‚æœ›': 2}

class LocalFinBERTEncoder:
    """ä½¿ç”¨æœ¬åœ°FinBERTæ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨"""
    def __init__(self, model_path=None):
        if model_path is None:
            model_path = r"d:\Learning\models\finbert-tone\models--yiyanghkust--finbert-tone\snapshots\4921590d3c0c3832c0efea24c8381ce0bda7844b"
        
        print(f"ğŸ“‚ å°è¯•åŠ è½½æœ¬åœ°FinBERTæ¨¡å‹: {model_path}")
        
        try:
            # æ£€æŸ¥transformersåº“
            import transformers
            print(f"âœ… Transformersåº“ç‰ˆæœ¬: {transformers.__version__}")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            import os
            required_files = ['config.json', 'pytorch_model.bin', 'vocab.txt', 'tokenizer_config.json']
            missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]
            
            if missing_files:
                raise FileNotFoundError(f"ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            
            from transformers import BertTokenizer, BertModel
            self.tokenizer = BertTokenizer.from_pretrained(model_path, local_files_only=True)
            self.model = BertModel.from_pretrained(model_path, local_files_only=True)
            
            # ç§»åŠ¨æ¨¡å‹åˆ°æ­£ç¡®è®¾å¤‡
            self.model = self.model.to(DEVICE)
            self.model.eval()
            print("âœ… æœ¬åœ°FinBERTæ¨¡å‹åŠ è½½æˆåŠŸ!")
            print(f"   æ¨¡å‹è®¾å¤‡: {next(self.model.parameters()).device}")
            print(f"   è¾“å‡ºç»´åº¦: {self.model.config.hidden_size}")
            self.use_finbert = True
            
        except ImportError as e:
            print(f"âŒ Transformersåº“æœªå®‰è£…: {e}")
            print("ğŸ’¡ è¯·è¿è¡Œ: conda install transformers")
            print("ğŸ”„ é™çº§åˆ°ç®€å•æ–‡æœ¬ç¼–ç å™¨...")
            self.use_finbert = False
            self._init_simple_encoder()
        except Exception as e:
            print(f"âŒ æœ¬åœ°FinBERTåŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ é™çº§åˆ°ç®€å•æ–‡æœ¬ç¼–ç å™¨...")
            self.use_finbert = False
            self._init_simple_encoder()
    
    def _init_simple_encoder(self):
        """åˆå§‹åŒ–ç®€å•ç¼–ç å™¨ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ - å¢å¼ºæ•°å€¼ç¨³å®šæ€§"""
        self.vocab_size = 1000
        self.embed_dim = 768
        self.vocab = self._build_vocab()
        # ç¡®ä¿embeddingsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼Œå¹¶ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–å€¼
        self.embeddings = torch.randn(self.vocab_size, self.embed_dim).to(DEVICE) * 0.1  # ç¼©å°åˆå§‹å€¼
        print(f"ğŸ”§ ç®€å•ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.embeddings.device}")
        
    def _build_vocab(self):
        """æ„å»ºåŸºç¡€é‡‘èè¯æ±‡è¡¨"""
        financial_words = [
            'ç¾è”å‚¨', 'åŠ æ¯', 'é™æ¯', 'é€šèƒ€', 'ç»æµ', 'å¢é•¿', 'ä¸‹è·Œ', 'ä¸Šæ¶¨',
            'ç¾å…ƒ', 'æ¬§å…ƒ', 'æ—¥å…ƒ', 'äººæ°‘å¸', 'è‹±é•‘', 'æ±‡ç‡', 'è´§å¸', 'æ”¿ç­–',
            'å¸‚åœº', 'æŠ•èµ„', 'é£é™©', 'æ”¶ç›Š', 'æ³¢åŠ¨', 'éœ‡è¡', 'è¶‹åŠ¿', 'é¢„æœŸ',
            'æ•°æ®', 'æŒ‡æ ‡', 'å°±ä¸š', 'CPI', 'GDP', 'è´¸æ˜“', 'å‡ºå£', 'è¿›å£',
            'é“¶è¡Œ', 'è‚¡å¸‚', 'å€ºåˆ¸', 'æœŸè´§', 'å¤–æ±‡', 'åŸºé‡‘', 'è¯åˆ¸', 'é‡‘è',
            'å±æœº', 'å¤è‹', 'è¡°é€€', 'ç¹è£', 'ç¨³å®š', 'åŠ¨è¡', 'æ”¹é©', 'å¼€æ”¾'
        ]
        vocab = ['<PAD>', '<UNK>'] + financial_words
        while len(vocab) < self.vocab_size:
            vocab.append(f'word_{len(vocab)}')
        return {word: idx for idx, word in enumerate(vocab)}
    
    def encode(self, text):
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡ - å¢å¼ºæ•°å€¼ç¨³å®šæ€§"""
        if self.use_finbert:
            try:
                inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=64, padding=True)
                # å°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    embedding = outputs.last_hidden_state[:,0,:].squeeze(0)  # [CLS] embedding
                    
                    # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œå¤„ç†
                    if torch.isnan(embedding).any() or torch.isinf(embedding).any():
                        print(f"âš ï¸ FinBERTè¾“å‡ºåŒ…å«NaN/Infï¼Œä½¿ç”¨ç®€å•ç¼–ç å™¨")
                        if not hasattr(self, 'vocab'):
                            self._init_simple_encoder()
                        return self._simple_encode(text)
                    
                    # L2æ ‡å‡†åŒ–ï¼Œé˜²æ­¢æ•°å€¼è¿‡å¤§
                    embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)
                    
                    return embedding
            except Exception as e:
                print(f"âš ï¸ FinBERTç¼–ç å¤±è´¥ï¼Œä½¿ç”¨ç®€å•ç¼–ç å™¨: {e}")
                # ç¡®ä¿ç®€å•ç¼–ç å™¨å·²åˆå§‹åŒ–
                if not hasattr(self, 'vocab'):
                    self._init_simple_encoder()
                return self._simple_encode(text)
        else:
            return self._simple_encode(text)
    
    def _simple_encode(self, text):
        """ç®€å•ç¼–ç æ–¹æ³• - å¢å¼ºæ•°å€¼ç¨³å®šæ€§"""
        words = []
        for char in text[:50]:  # é™åˆ¶é•¿åº¦
            if char in self.vocab:
                words.append(self.vocab[char])
            else:
                words.append(self.vocab['<UNK>'])
        
        if not words:
            # è¿”å›å°çš„éšæœºå‘é‡è€Œä¸æ˜¯å¤§çš„
            result = torch.randn(self.embed_dim).to(DEVICE) * 0.01
        else:
            word_embeddings = self.embeddings[words]
            result = torch.mean(word_embeddings, dim=0)
        
        # L2æ ‡å‡†åŒ–ï¼Œç¡®ä¿æ•°å€¼ç¨³å®š
        result = torch.nn.functional.normalize(result, p=2, dim=0)
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(result).any() or torch.isinf(result).any():
            print(f"âš ï¸ ç®€å•ç¼–ç å™¨è¾“å‡ºå¼‚å¸¸ï¼Œä½¿ç”¨é›¶å‘é‡")
            result = torch.zeros(self.embed_dim).to(DEVICE)
        
        return result

def extract_struct_features(sample):
    """æå–ç»“æ„åŒ–ç‰¹å¾ - å¢å¼ºæ•°å€¼ç¨³å®šæ€§"""
    keys = ['quantity', 'proportion', 'valueAtRisk', 'beta', 'daily_volatility', 'sentiment_score']
    values = []
    for k in keys:
        value = sample.get(k, 0)
        if isinstance(value, (int, float)):
            # æ£€æŸ¥å¹¶å¤„ç†å¼‚å¸¸å€¼
            if np.isnan(value) or np.isinf(value):
                value = 0.0
            
            # æ•°å€¼æ ‡å‡†åŒ–å’Œé™åˆ¶èŒƒå›´ï¼Œé˜²æ­¢æå€¼
            if k == 'quantity':
                # æ•°é‡æ ‡å‡†åŒ–åˆ°[-1, 1]
                value = np.tanh(value / 10000.0)
            elif k == 'proportion':
                # æ¯”ä¾‹é™åˆ¶åˆ°[0, 1]
                value = np.clip(value, 0.0, 1.0)
            elif k == 'valueAtRisk':
                # VaRæ ‡å‡†åŒ–åˆ°[-1, 1]
                value = np.tanh(value * 10)  # æ”¾å¤§åtanh
            elif k == 'beta':
                # Betaæ ‡å‡†åŒ–åˆ°[-1, 1]
                value = np.tanh(value / 2.0)
            elif k == 'daily_volatility':
                # æ³¢åŠ¨ç‡æ ‡å‡†åŒ–åˆ°[0, 1]
                value = np.clip(value * 50, 0.0, 1.0)  # æ”¾å¤§åclip
            elif k == 'sentiment_score':
                # æƒ…æ„Ÿåˆ†æ•°é™åˆ¶åˆ°[-1, 1]
                value = np.clip(value, -1.0, 1.0)
            
            values.append(float(value))
        else:
            values.append(0.0)
    
    result = np.array(values, dtype=np.float32)
    
    # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰NaNæˆ–Inf
    if np.isnan(result).any() or np.isinf(result).any():
        print(f"âš ï¸ ç»“æ„åŒ–ç‰¹å¾åŒ…å«å¼‚å¸¸å€¼ï¼Œä½¿ç”¨é›¶å€¼æ›¿æ¢")
        result = np.zeros(len(keys), dtype=np.float32)
    
    return result

class OfflineFinDataset(Dataset):
    """ç¦»çº¿é‡‘èæ•°æ®é›†"""
    def __init__(self, data, seq_len=10, use_dual_mamba=True):
        self.data = self.prepare_sequences(data, seq_len)
        self.text_encoder = LocalFinBERTEncoder()  # ä½¿ç”¨æœ¬åœ°FinBERTç¼–ç å™¨
        self.use_dual_mamba = use_dual_mamba
        print(f"ğŸ“Š æ•°æ®é›†æ„å»ºå®Œæˆï¼Œå…±{len(self.data)}ä¸ªåºåˆ—")
        
    def prepare_sequences(self, data, seq_len):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        df = pd.DataFrame(data)
        df['datetime'] = pd.to_datetime(df['date'])
        df = df.sort_values('datetime')
        
        sequences = []
        for i in range(len(df) - seq_len + 1):
            seq_data = df.iloc[i:i+seq_len].to_dict('records')
            sequences.append(seq_data)
        
        return sequences
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sequence = self.data[idx]
        
        if self.use_dual_mamba:
            # åŒMambaåºåˆ—æ•°æ®
            news_feats = []
            price_feats = []
            timestamps = []
            
            for sample in sequence:
                # æ–‡æœ¬ç‰¹å¾
                text_feat = self.text_encoder.encode(sample['news'])
                news_feats.append(text_feat)
                
                # ç»“æ„åŒ–ç‰¹å¾
                struct_feat = extract_struct_features(sample)
                price_feats.append(struct_feat)
                
                # æ—¶é—´æˆ³
                timestamp = pd.to_datetime(sample['date']).timestamp() / (24 * 3600)
                timestamps.append(timestamp)
            
            # æ ‡ç­¾æ¥è‡ªæœ€åä¸€ä¸ªæ ·æœ¬
            last_sample = sequence[-1]
            action = ACTION2IDX.get(last_sample['action'], 2)
            hedge = float(last_sample['hedge_ratio'])
            
            return {
                'news_feats': torch.stack(news_feats),
                'price_feats': torch.tensor(np.stack(price_feats), dtype=torch.float32),
                'timestamps': torch.tensor(timestamps, dtype=torch.long),
                'action': action,
                'hedge': hedge
            }
        else:
            # ç®€å•æ¨¡å¼
            sample = sequence[-1]
            text_feat = self.text_encoder.encode(sample['news'])
            struct_feat = extract_struct_features(sample)
            action = ACTION2IDX.get(sample['action'], 2)
            hedge = float(sample['hedge_ratio'])
            return text_feat, struct_feat, action, hedge

def collate_fn_dual_mamba(batch):
    """åŒMambaæ‰¹å¤„ç†"""
    news_feats = torch.stack([item['news_feats'] for item in batch])
    price_feats = torch.stack([item['price_feats'] for item in batch])
    news_timestamps = torch.stack([item['timestamps'] for item in batch])
    price_timestamps = torch.stack([item['timestamps'] for item in batch])  # å‡è®¾åŒæ ·çš„æ—¶é—´æˆ³
    actions = torch.tensor([item['action'] for item in batch], dtype=torch.long)
    hedges = torch.tensor([item['hedge'] for item in batch], dtype=torch.float32).unsqueeze(1)
    
    return news_feats, price_feats, news_timestamps, price_timestamps, actions, hedges

def collate_fn_simple(batch):
    """ç®€å•æ¨¡å‹æ‰¹å¤„ç†"""
    text_feats, struct_feats, actions, hedges = zip(*batch)
    text_feats = torch.stack(text_feats)
    struct_feats = torch.tensor(np.stack(struct_feats), dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.long)
    hedges = torch.tensor(hedges, dtype=torch.float32).unsqueeze(1)
    return text_feats, struct_feats, actions, hedges

def load_data(path):
    """åŠ è½½JSONæ•°æ®"""
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def train_dual_mamba_offline():
    """ç¦»çº¿è®­ç»ƒåŒMambaæ¨¡å‹ - å¸¦è¿›åº¦æ¡å’ŒGPUä¼˜åŒ–"""
    print("ğŸš€ å¼€å§‹ç¦»çº¿è®­ç»ƒåŒMambaæ¨¡å‹...")
    start_time = time.time()
    
    # æ£€æŸ¥è®¾å¤‡å’ŒGPUå†…å­˜
    print(f"ğŸ¯ è®¾å¤‡: {DEVICE}")
    if DEVICE == 'cuda':
        print(f"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
        # æ¸…ç©ºGPUç¼“å­˜
        torch.cuda.empty_cache()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®...")
    data = load_data(DATA_PATH)
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(data)}æ¡è®°å½•")
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ”§ æ­£åœ¨æ„å»ºæ•°æ®é›†...")
    dataset = OfflineFinDataset(data, seq_len=5, use_dual_mamba=True)  # é™ä½åºåˆ—é•¿åº¦
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, 
                       collate_fn=collate_fn_dual_mamba, num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
                       pin_memory=False)  # ç¦ç”¨pin_memoryä»¥é¿å…CUDAå¼ é‡é”™è¯¯
    
    # åˆ›å»ºæ¨¡å‹ - é™ä½å¤æ‚åº¦æé«˜æ•°å€¼ç¨³å®šæ€§
    print("ğŸ—ï¸ æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
    model = DualMambaModel(
        text_dim=768,
        struct_dim=6,  # ä¿®æ­£ç‰¹å¾ç»´åº¦
        d_mamba=128,  # é™ä½éšè—å±‚ç»´åº¦
        n_mamba_layers=2,  # å‡å°‘å±‚æ•°
        num_actions=3
    ).to(DEVICE)
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åŒ…å«NaN
    print("ğŸ” æ£€æŸ¥æ¨¡å‹å‚æ•°...")
    nan_params = []
    for name, param in model.named_parameters():
        if torch.isnan(param).any() or torch.isinf(param).any():
            nan_params.append(name)
    
    if nan_params:
        print(f"âš ï¸ å‘ç°NaN/Infå‚æ•°: {nan_params}")
        print("ğŸ”„ é‡æ–°åˆå§‹åŒ–è¿™äº›å‚æ•°...")
        with torch.no_grad():
            for name, param in model.named_parameters():
                if name in nan_params:
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
    else:
        print("âœ… æ¨¡å‹å‚æ•°æ£€æŸ¥é€šè¿‡")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion_action = nn.CrossEntropyLoss()
    criterion_hedge = nn.MSELoss()
    # ä½¿ç”¨æ›´ä¿å®ˆçš„ä¼˜åŒ–å™¨å‚æ•°
    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4, eps=1e-8, betas=(0.9, 0.999))
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ›´ä¿å®ˆçš„è°ƒæ•´
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = None
    if DEVICE == 'cuda' and MIXED_PRECISION:
        try:
            # æ–°ç‰ˆæœ¬PyTorchçš„æ¨èæ–¹å¼
            scaler = torch.amp.GradScaler('cuda')
        except:
            # å›é€€åˆ°æ—§ç‰ˆæœ¬æ–¹å¼
            scaler = torch.cuda.amp.GradScaler()
        print("âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    else:
        print("ğŸ”§ ä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ (æ›´ç¨³å®š)")
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    print(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   è®¾å¤‡: {DEVICE}")
    print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   æ€»æ‰¹æ¬¡æ•°: {len(loader)}")
    print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")
    print(f"   å­¦ä¹ ç‡: {LR}")
    print(f"   è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"   æ··åˆç²¾åº¦: {MIXED_PRECISION and DEVICE == 'cuda'}")
    print("-" * 60)
    
    # è®­ç»ƒå†å²è®°å½•
    train_history = {
        'epoch': [],
        'loss': [],
        'action_acc': [],
        'time_per_epoch': [],
        'gpu_memory': []
    }
    
    # æ—©æœŸåœæ­¢æœºåˆ¶
    best_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 5
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        total_loss = 0
        total_action_acc = 0
        num_batches = 0
        optimizer.zero_grad()
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(loader, desc=f'Epoch {epoch+1}/{EPOCHS}', 
                   unit='batch', ncols=120, 
                   bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
        
        try:
            for batch_idx, (news_feat, price_feat, news_ts, price_ts, action, hedge) in enumerate(pbar):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                news_feat = news_feat.to(DEVICE, non_blocking=True)
                price_feat = price_feat.to(DEVICE, non_blocking=True)
                news_ts = news_ts.to(DEVICE, non_blocking=True)
                price_ts = price_ts.to(DEVICE, non_blocking=True)
                action = action.to(DEVICE, non_blocking=True)
                hedge = hedge.to(DEVICE, non_blocking=True)
                
                # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaN
                if torch.isnan(news_feat).any() or torch.isnan(price_feat).any():
                    print(f"âš ï¸ è¾“å…¥æ•°æ®åŒ…å«NaNï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                    continue
                if torch.isnan(hedge).any() or torch.isnan(action.float()).any():
                    print(f"âš ï¸ æ ‡ç­¾æ•°æ®åŒ…å«NaNï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                    continue
                
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                if scaler is not None:
                    with torch.amp.autocast('cuda'):  # ä¿®å¤å¼ƒç”¨è­¦å‘Š
                        action_logits, hedge_pred = model(news_feat, price_feat, news_ts, price_ts)
                        loss_action = criterion_action(action_logits, action)
                        loss_hedge = criterion_hedge(hedge_pred, hedge)
                        loss = (loss_action + 0.5 * loss_hedge) / GRADIENT_ACCUMULATION_STEPS
                        
                        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                        if torch.isnan(loss) or torch.isinf(loss):
                            print(f"âš ï¸ æ£€æµ‹åˆ°NaN/InfæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                            continue
                else:
                    action_logits, hedge_pred = model(news_feat, price_feat, news_ts, price_ts)
                    loss_action = criterion_action(action_logits, action)
                    loss_hedge = criterion_hedge(hedge_pred, hedge)
                    loss = (loss_action + 0.5 * loss_hedge) / GRADIENT_ACCUMULATION_STEPS
                    
                    # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"âš ï¸ æ£€æµ‹åˆ°NaN/InfæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                
                # åå‘ä¼ æ’­
                if scaler is not None:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
                    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaN
                    has_nan_grad = False
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                                print(f"âš ï¸ æ£€æµ‹åˆ°{name}ä¸­çš„NaN/Infæ¢¯åº¦")
                                has_nan_grad = True
                                break
                    
                    if has_nan_grad:
                        print("âš ï¸ æ¢¯åº¦åŒ…å«NaN/Infï¼Œè·³è¿‡ä¼˜åŒ–æ­¥éª¤")
                        optimizer.zero_grad()
                        continue
                    
                    # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                    total_norm = 0
                    for param in model.parameters():
                        if param.grad is not None:
                            param_norm = param.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** (1. / 2)
                    
                    # å¦‚æœæ¢¯åº¦è¿‡å¤§ï¼Œé¢å¤–æ‰“å°è­¦å‘Š
                    if total_norm > 10.0:
                        print(f"âš ï¸ æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {total_norm:.4f}")
                    
                    # æ·»åŠ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                    if scaler is not None:
                        scaler.unscale_(optimizer)  # å…ˆè§£é™¤ç¼©æ”¾
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
                        optimizer.step()
                    optimizer.zero_grad()
                
                # ç´¯è®¡æŒ‡æ ‡
                total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
                
                # è®¡ç®—å‡†ç¡®ç‡
                pred_actions = torch.argmax(action_logits, dim=1)
                batch_acc = (pred_actions == action).float().mean().item()
                total_action_acc += batch_acc
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                avg_loss = total_loss / num_batches
                avg_acc = total_action_acc / num_batches
                
                # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                gpu_mem = ""
                if DEVICE == 'cuda':
                    gpu_mem = f"GPU:{torch.cuda.memory_allocated()/1024**3:.1f}GB"
                
                pbar.set_postfix({
                    'Loss': f'{avg_loss:.4f}',
                    'Acc': f'{avg_acc:.3f}',
                    'LR': f'{optimizer.param_groups[0]["lr"]:.2e}',
                    'Mem': gpu_mem
                })
                
                # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                if (batch_idx + 1) % 50 == 0:
                    elapsed = time.time() - epoch_start_time
                    eta = elapsed / (batch_idx + 1) * (len(loader) - batch_idx - 1)
                    print(f"\n   ğŸ“ Batch {batch_idx+1}/{len(loader)} | "
                          f"Loss: {avg_loss:.4f} | Acc: {avg_acc:.3f} | "
                          f"ETA: {eta:.1f}s | {gpu_mem}")
                
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            continue
        
        # å…³é—­è¿›åº¦æ¡
        pbar.close()
        
        # è®¡ç®—epochæŒ‡æ ‡
        epoch_time = time.time() - epoch_start_time
        if num_batches > 0:
            avg_loss = total_loss / num_batches
            avg_acc = total_action_acc / num_batches
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_loss)
            
            # æ—©æœŸåœæ­¢æ£€æŸ¥
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_path = 'dual_mamba_offline_best.pth'
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': avg_loss,
                    'accuracy': avg_acc,
                }, best_model_path)
                print(f"ğŸ’¾ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
            else:
                patience_counter += 1
                print(f"â³ æ—©æœŸåœæ­¢è®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")
            
            # è®°å½•å†å²
            train_history['epoch'].append(epoch + 1)
            train_history['loss'].append(avg_loss)
            train_history['action_acc'].append(avg_acc)
            train_history['time_per_epoch'].append(epoch_time)
            if DEVICE == 'cuda':
                train_history['gpu_memory'].append(torch.cuda.max_memory_allocated()/1024**3)
            
            # æ‰“å°epochæ€»ç»“
            print(f"\nğŸ¯ Epoch {epoch+1}/{EPOCHS} å®Œæˆ:")
            print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"   åŠ¨ä½œå‡†ç¡®ç‡: {avg_acc:.3f}")
            print(f"   å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            print(f"   è€—æ—¶: {epoch_time:.1f}ç§’")
            print(f"   ä¼°è®¡å‰©ä½™æ—¶é—´: {epoch_time * (EPOCHS - epoch - 1):.1f}ç§’")
            if DEVICE == 'cuda':
                print(f"   æœ€å¤§GPUå†…å­˜ä½¿ç”¨: {torch.cuda.max_memory_allocated()/1024**3:.1f}GB")
                torch.cuda.reset_peak_memory_stats()
            print("-" * 60)
            
            # æ—©æœŸåœæ­¢æ£€æŸ¥
            if patience_counter >= early_stop_patience:
                print(f"ğŸ›‘ æ—©æœŸåœæ­¢: æŸå¤±åœ¨{early_stop_patience}ä¸ªepochå†…æœªæ”¹å–„")
                break
        
        # æ¯5ä¸ªepochä¿å­˜æ¨¡å‹
        if (epoch + 1) % 5 == 0:
            save_path = f'dual_mamba_offline_epoch_{epoch+1}.pth'
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': avg_loss,
                'accuracy': avg_acc,
            }, save_path)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")
        
        # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
        sys.stdout.flush()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = 'dual_mamba_offline_final.pth'
    
    # ç¡®ä¿æœ‰è®­ç»ƒå†å²æ•°æ®
    if train_history['loss']:
        final_loss = train_history['loss'][-1]
        final_accuracy = train_history['action_acc'][-1]
    else:
        final_loss = 0.0
        final_accuracy = 0.0
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰å®Œæˆä»»ä½•è®­ç»ƒepoch")
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_history': train_history,
        'final_loss': final_loss,
        'final_accuracy': final_accuracy,
    }, final_path)
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    total_time = time.time() - start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š è®­ç»ƒæ€»ç»“:")
    print(f"   æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    
    if train_history['loss']:
        print(f"   æœ€ç»ˆæŸå¤±: {train_history['loss'][-1]:.4f}")
        print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {train_history['action_acc'][-1]:.3f}")
        print(f"   å¹³å‡æ¯epochæ—¶é—´: {np.mean(train_history['time_per_epoch']):.1f}ç§’")
        if DEVICE == 'cuda' and train_history['gpu_memory']:
            print(f"   å¹³å‡GPUå†…å­˜ä½¿ç”¨: {np.mean(train_history['gpu_memory']):.1f}GB")
    else:
        print("   âš ï¸ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œæœªå®Œæˆä»»ä½•epoch")
    
    print(f"   æ¨¡å‹å·²ä¿å­˜: {final_path}")
    print("âœ… ç¦»çº¿åŒMambaæ¨¡å‹è®­ç»ƒå®Œæˆ!")

def train_simple_offline():
    """ç¦»çº¿è®­ç»ƒç®€å•æ¨¡å‹"""
    print("ğŸ”„ å¼€å§‹ç¦»çº¿è®­ç»ƒç®€å•æ¨¡å‹...")
    
    data = load_data(DATA_PATH)
    dataset = OfflineFinDataset(data, use_dual_mamba=False)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_simple)
    
    model = MultimodalDecisionModel(text_dim=768, struct_dim=6).to(DEVICE)
    criterion_action = nn.CrossEntropyLoss()
    criterion_hedge = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for text_feat, struct_feat, action, hedge in loader:
            text_feat = text_feat.to(DEVICE)
            struct_feat = struct_feat.to(DEVICE)
            action = action.to(DEVICE)
            hedge = hedge.to(DEVICE)
            
            optimizer.zero_grad()
            action_logits, hedge_pred = model(text_feat, struct_feat)
            loss_action = criterion_action(action_logits, action)
            loss_hedge = criterion_hedge(hedge_pred, hedge)
            loss = loss_action + loss_hedge
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(loader):.4f}')
    
    torch.save(model.state_dict(), 'simple_model_offline.pth')
    print("âœ… ç¦»çº¿ç®€å•æ¨¡å‹è®­ç»ƒå®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ ç¦»çº¿è®­ç»ƒæ¨¡å¼å¯åŠ¨")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {DATA_PATH}")
    
    if USE_DUAL_MAMBA:
        train_dual_mamba_offline()
    else:
        train_simple_offline()

if __name__ == '__main__':
    main()
